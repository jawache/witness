# Witness Development Log

This log chronicles the development journey of Witness, an Obsidian plugin that implements the Model Context Protocol (MCP) to enable AI-assisted vault management.

---

## 2026-01-30 - Phase 1: MCP Server Implementation

**Objective**: Create an Obsidian plugin that acts as an MCP server running inside Obsidian, enabling Claude Desktop to interact with the vault.

### What We Built

Implemented a complete MCP server with 6 core tools:

1. **read_file** - Read any file from the vault
2. **write_file** - Create or modify files
3. **list_files** - Browse directory contents
4. **edit_file** - Find and replace text (surgical edits)
5. **search** - Full-text search across all markdown files
6. **execute_command** - Execute any Obsidian command by ID

### The Journey

**Morning: Architecture Decision**

Started with a critical architectural choice: Should the plugin BE the MCP server (Approach A) or should it be an external server connecting via REST API (Approach B)?

Decision: **Approach A** - Plugin runs the MCP server directly inside Obsidian. This means:
- Zero dependencies on other plugins
- Direct access to Obsidian's Vault API
- HTTP server running in Electron process
- More complex but more powerful

**Early Development: Initial Implementation**

First attempt was manual JSON-RPC message handling. Messy and error-prone. Quickly pivoted to using the official `@modelcontextprotocol/sdk` package for proper protocol compliance.

Initial tools: read_file, write_file, list_files working with manual testing via curl.

**The Great Session Management Mystery**

Hit a major blocker: "Server already initialized" error on every connection attempt from Claude Desktop. Spent considerable time investigating:

1. **Red Herring #1**: Node version issues
   - mcp-remote showed "File is not defined" errors
   - Tried forcing Node v24 via PATH environment
   - Turned out to be irrelevant - still works with Node v18

2. **The Real Problem**: Session Management
   - Was creating a new transport for EVERY HTTP request
   - Should create ONE transport per SESSION
   - Maintain a `Map<sessionId, transport>`
   - Only call `mcpServer.connect(transport)` once per session
   - Reuse transport for all subsequent requests in that session

**Breakthrough**: Found the pattern in SDK examples at `node_modules/@modelcontextprotocol/sdk/dist/esm/examples/server/simpleStreamableHttp.js`. The key insight: StreamableHTTP is stateful. Sessions span multiple HTTP requests.

**Second Major Issue**: SSE Stream 404 Errors

Tools were listing successfully, but then Claude Desktop couldn't establish the SSE (Server-Sent Events) stream for receiving notifications.

Problem: Only handling POST requests to `/mcp`, not GET requests.

Solution: StreamableHTTP uses dual endpoints:
- POST for sending JSON-RPC messages
- GET for SSE streams (receiving)

Also had to handle GET requests differently - no body to parse.

**Late Afternoon: Completing the Tool Set**

With the connection working, implemented the remaining three tools:

1. **edit_file**: Find/replace with proper regex escaping for literal matching
2. **search**: Full-text search with optional case sensitivity and path filtering
3. **execute_command**: Direct access to Obsidian's command system

All tools use Zod schemas for parameter validation, letting the SDK auto-generate JSON Schema.

### Technical Achievements

- **Proper Session Management**: Map-based transport tracking
- **SSE Support**: Bidirectional communication working
- **Type Safety**: Zod validation for all parameters
- **Error Handling**: Clear error messages with helpful suggestions
- **Zero External Dependencies**: No REST API plugins needed

### Documentation Created

- **README.md**: User-facing installation and usage guide
- **CLAUDE.md**: Technical implementation details for AI assistants
- **PHASE1-COMPLETE.md**: Comprehensive tool reference with examples
- **test-tools.sh**: Testing verification script
- **DEVLOG**: This file

### Key Learnings

1. **StreamableHTTP is stateful**: Don't create new transports per request
2. **Dual endpoints matter**: Handle both POST and GET for full functionality
3. **Read the SDK examples**: The answers are often in the source code
4. **Test methodically**: Health endpoint + logs + screenshots = debugging trinity
5. **Document as you go**: Future you will thank present you

### Statistics

- **4 commits** in main branch
- **452 lines** of TypeScript
- **6 tools** implemented
- **100%** Phase 1 objectives met

### Next Steps

Phase 2: Remote Access
- Cloudflare Tunnel integration
- WhatsApp/Telegram bot connection
- Mobile support
- Multi-user authentication
- Security hardening

### Reflections

The session management bug was frustrating but educational. It reinforced the importance of:
- Reading official examples thoroughly
- Not assuming the first error message is the real problem
- Systematic debugging rather than guessing
- Writing documentation that captures the "why" not just the "what"

Phase 1 is complete and working beautifully. The foundation is solid for Phase 2 expansion.

---

## 2026-01-31 - Integration Test Suite

**Objective**: Build automated testing infrastructure to verify MCP tools work correctly end-to-end.

### What We Built

A complete integration test suite using Vitest that tests all MCP tools against a running Obsidian instance.

**Test Infrastructure:**
- `test/vault/` - Minimal, stable test vault with known fixed content
- `test/integration/mcp-client.ts` - MCP protocol client for testing
- `test/integration/mcp.test.ts` - 17 integration tests covering all tools
- `test/vitest.config.ts` - Test configuration

**Vault Reorganization:**
- Renamed `test-vault/` → `demo-vault/` for manual testing and development
- Created `test/vault/` for automated tests (port 3001)
- Clear separation: demo vault can change freely, test vault stays stable

**NPM Scripts:**
- `npm test` - Run integration tests
- `npm run test:watch` - Watch mode
- `npm run test:start-obsidian` - Launch Obsidian with test vault
- `npm run test:install-plugin` - Build and install plugin

### The Journey

**Design Decision: Integration vs Unit Tests**

Considered two approaches:
1. Mock-based unit tests - Fast but doesn't test real integration
2. Integration tests against running Obsidian - Slower but tests actual behavior

Chose integration tests because:
- Most plugin logic is tightly coupled to Obsidian's API
- Mocking the vault would be complex and fragile
- We need to verify files are actually created/modified
- MCP protocol nuances are hard to unit test

**Test Verification Strategy**

Made sure tests actually verify results, not just check for errors:
- `write_file`: Writes content, then reads it back to verify
- `edit_file`: Performs edit, then reads file to confirm change
- `read_file`: Checks returned content matches known file content
- `search`: Verifies correct files appear in results

**Initial Test Failures**

First run: 14/17 tests passed. The 3 failures were because MCP returns errors via `isError: true` flag rather than throwing exceptions. Updated tests to check `result.isError` instead of `expect(...).rejects.toThrow()`.

Second run: 17/17 tests pass.

### Technical Achievements

- **True End-to-End Testing**: Tests run against real Obsidian instance
- **All Tools Covered**: 17 tests for 8 MCP tools
- **Error Case Coverage**: Tests both success and error paths
- **Result Verification**: Tests read back data to verify operations worked
- **Clean Separation**: Test vault isolated from demo/development vault

### Test Coverage Summary

| Tool | Tests | Verification |
|------|-------|--------------|
| read_file | 2 | Content matches known file |
| write_file | 2 | Read back verifies content |
| edit_file | 2 | Read back verifies changes |
| list_files | 2 | Expected files present |
| search | 2 | Correct files in results |
| find_files | 2 | Pattern matching works |
| get_orientation | 1 | Returns orientation doc |
| execute_command | 2 | Success/error handling |

### Key Learnings

1. **MCP error handling**: Errors return `isError: true`, not exceptions
2. **TypeScript config**: Need to exclude test directory from main build
3. **Vitest config**: `sequence.concurrent: false` for sequential tests
4. **Port separation**: Test vault uses 3001 to avoid conflicts with demo

### Statistics

- **17 tests** all passing
- **166ms** test duration
- **2 vaults** (demo + test)
- **4 npm scripts** for testing workflow

### Next Steps

- Consider adding GitHub Actions CI
- Add more edge case tests as bugs are found
- Potential: Screenshot tests for UI verification

---

## 2026-01-31 - File-Based MCP Logging

**Objective**: Implement file-based logging so logs are accessible without Obsidian's Developer Console.

### What We Built

A logging system that writes MCP server logs to files in the plugin directory.

**MCPLogger Class:**

- Writes to both console AND file simultaneously
- Buffered writes (flushes every 1 second or 50 entries)
- Date-based log files: `mcp-YYYY-MM-DD.log`
- Log levels: INFO, ERROR, DEBUG, MCP
- ISO timestamps for all entries
- Graceful shutdown flushes remaining logs

**Log Location:**

```text
.obsidian/plugins/witness/logs/mcp-2026-01-31.log
```

**Log Format:**

```text
[2026-01-31T17:49:56.003Z] [INFO] Witness plugin loaded
[2026-01-31T17:50:07.214Z] [MCP] POST /mcp
[2026-01-31T17:50:07.242Z] [MCP] read_file called with path: "test.md"
```

### Benefits

1. **Debugging**: Claude Code can read logs directly to help debug issues
2. **Bug Reports**: Users can easily share logs when reporting problems
3. **No Console Required**: No need to open Developer Console to see what's happening
4. **History**: Date-based files provide historical record

### Testing

Added 2 new integration tests (now 19 total):

1. **should write logs to file** - Verifies log file exists and contains expected entries
2. **should log error cases** - Verifies failed operations are logged

### Implementation Details

Replaced all `console.log` and `console.error` calls with `this.logger.*` calls:

- `this.logger.info()` - General information
- `this.logger.error()` - Error conditions
- `this.logger.debug()` - Debug info
- `this.logger.mcp()` - MCP protocol-level logging

The logger buffers writes to avoid I/O overhead on every log call.

### Statistics

- **2 new tests** added
- **19 tests** total, all passing
- **~150 lines** of logging code
- **1 new class** (MCPLogger)

### Next Steps

- Implement `move_file` tool for moving/renaming files
- Consider log rotation for large vaults

---

## 2026-01-31 - Move/Rename File Tool

**Objective**: Implement `move_file` MCP tool to move or rename files within the vault.

### What We Built

A new `move_file` MCP tool that enables AI assistants to move or rename files within the vault using Obsidian's native `vault.rename()` API.

**Tool Implementation:**

- Takes `from` (source path) and `to` (destination path) parameters
- Uses Obsidian's `vault.rename()` for proper file system handling
- Automatically creates parent directories if needed
- Error handling for: source not found, destination already exists
- Marked as destructive (destructiveHint: true)
- Full logging integration

**Test Coverage:**

Added 4 new integration tests (23 total):
1. Should move/rename a file
2. Should move file to subfolder
3. Should return error for non-existent source
4. Should return error if destination exists

### Technical Details

```typescript
this.mcpServer.tool(
  'move_file',
  'Move or rename a file within the vault',
  {
    from: z.string().describe('Current path of the file'),
    to: z.string().describe('New path for the file'),
  },
  { destructiveHint: true },
  async ({ from, to }) => {
    const file = this.app.vault.getAbstractFileByPath(from);
    if (!file) throw new Error(`Source file not found: ${from}`);

    const existing = this.app.vault.getAbstractFileByPath(to);
    if (existing) throw new Error(`Destination already exists: ${to}`);

    // Create parent directories if needed
    const destDir = to.substring(0, to.lastIndexOf('/'));
    if (destDir) {
      const parentFolder = this.app.vault.getAbstractFileByPath(destDir);
      if (!parentFolder) {
        await this.app.vault.createFolder(destDir);
      }
    }

    await this.app.vault.rename(file, to);
    return { content: [{ type: 'text', text: `Moved ${from} to ${to}` }] };
  }
);
```

### Key Features

- **Preserves Metadata**: Unlike delete + create, rename preserves file creation date and other metadata
- **Parent Directory Creation**: Automatically creates destination directories if they don't exist
- **Conflict Prevention**: Fails safely if destination already exists
- **Full Logging**: Integrated with MCPLogger for debugging

### Statistics

- **4 new tests** added
- **23 tests** total, all passing
- **~50 lines** of new tool code
- **9 MCP tools** now available

### Next Steps

- Remote Access via Cloudflare Tunnel (Phase 2)

---

## 2026-01-31 - Cloudflare Quick Tunnel Integration

**Objective**: Enable remote access to the Witness MCP server via Cloudflare's Quick Tunnel feature.

### What We Built

A complete tunneling solution that exposes the local MCP server to the internet, allowing Claude to connect from anywhere.

**Key Features:**

- **Zero-Config Tunneling**: Toggle "Enable Quick Tunnel" in settings and get a public URL
- **Auto-Install**: cloudflared binary automatically downloads on first use to `~/.witness/bin/`
- **Settings UI**: Display current URL with copy button, regenerate button, status indicator
- **Notifications**: Toast notification when tunnel connects with URL

**Architecture:**

```text
Claude (anywhere)
    ↓ HTTPS
Cloudflare Edge (random-words.trycloudflare.com)
    ↓ Tunnel
cloudflared binary (running on user's machine)
    ↓ HTTP
Witness Plugin (localhost:3000)
    ↓
Obsidian Vault
```

### The Journey

**Challenge: Binary Path Resolution**

Initial attempt failed with:

```text
spawn /Applications/Obsidian.app/Contents/Resources/electron.asar/bin/cloudflared ENOENT
```

The cloudflared npm package couldn't find its binary inside Obsidian's Electron environment. The package looks for the binary relative to its module location, but bundling breaks this.

**Solution: Custom Binary Management**

1. Install cloudflared to a known, stable location: `~/.witness/bin/cloudflared`
2. Use the package's `use()` function to point to our binary location
3. Handle first-run installation automatically

```typescript
private getCloudflaredBinPath(): string {
  const binDir = path.join(os.homedir(), '.witness', 'bin');
  return path.join(binDir, 'cloudflared');
}

await installCloudflared(binPath);  // Install to our location
useCloudflared(binPath);            // Tell package where to find it
```

**esbuild Configuration**

Also needed to update `esbuild.config.mjs` to handle `node:` prefixed module specifiers:

```javascript
const nodeBuiltins = builtins.flatMap(m => [m, `node:${m}`]);
// Add platform: "node" for proper Node.js target
```

### Implementation Details

**Settings Interface:**

```typescript
interface WitnessSettings {
  // ... existing
  enableTunnel: boolean;
  tunnelUrl: string | null;
}
```

**Tunnel Lifecycle:**

1. On plugin load: Start tunnel if enabled
2. On plugin unload: Stop tunnel gracefully
3. Regenerate: Stop, wait, restart

**Event Handling:**

```typescript
const tunnel = Tunnel.quick(localUrl);
tunnel.once('url', async (url) => {
  this.settings.tunnelUrl = url;
  await this.saveSettings();
  new Notice(`Tunnel connected: ${url}/mcp`);
});
tunnel.once('connected', (conn) => {
  this.logger.info(`Connected to ${conn.location}`);
});
tunnel.on('exit', (code) => {
  this.tunnelProcess = null;
  this.tunnelStatus = 'disconnected';
});
```

### Testing

Verified end-to-end:

```bash
# Health check through tunnel
curl https://bought-funky-charges-reservation.trycloudflare.com/health
# {"status":"ok","plugin":"witness"}

# MCP initialize through tunnel
curl -X POST .../mcp -H "Accept: application/json, text/event-stream" -d '...'
# {"result":{"protocolVersion":"2024-11-05",...}}
```

### Key Learnings

1. **Electron bundling breaks module paths**: Can't rely on npm package default paths inside Obsidian
2. **Quick Tunnels are truly zero-config**: No Cloudflare account needed
3. **Binary installation is idempotent**: Package handles version checking
4. **URL changes on restart**: Users must reconfigure Claude after Obsidian restart

### Statistics

- **1 npm package** added (cloudflared)
- **~150 lines** of tunnel code
- **4 new settings** (enableTunnel, tunnelUrl, status, UI elements)
- **~50MB** cloudflared binary (downloaded on first use)

### Next Steps

- Authentication token enforcement for tunnel
- WhatsApp/Telegram bot integration
- Permanent URLs via Named Tunnel (documentation)

---

## 2026-02-01 - Token Authentication for Remote Access

**Objective**: Implement simple token-based authentication to protect the MCP endpoint when exposed via Cloudflare tunnel.

### What We Built

A simplified authentication system that protects remote access with a token, replacing the initial OAuth 2.0 implementation that proved incompatible with Claude Desktop.

**Key Features:**

- **Query Parameter Token**: Access via `?token=xxx` in the URL
- **Authorization Header**: Also accepts `Authorization: Bearer xxx`
- **Auto-Generate Token**: Token created automatically when enabling auth
- **Settings UI**: Token field with regenerate button under Remote Access section
- **URL Integration**: MCP URL displays with token included when auth is enabled

### The Journey

**Initial Approach: OAuth 2.0 Client Credentials**

Started implementing full OAuth 2.0 Client Credentials Grant:
- Added `/.well-known/oauth-authorization-server` metadata endpoint
- Implemented `/oauth/token` endpoint
- Generated client ID and secret on first run
- Token issuance with expiration

**The Problem: Claude Desktop OAuth Requirements**

Discovered through web research that Claude Desktop requires OAuth 2.1 Authorization Code flow with PKCE, not the simpler Client Credentials grant we implemented. Claude's MCP authentication is designed for delegated authorization scenarios, not machine-to-machine authentication.

Key findings:
- Claude expects a full OAuth 2.1 flow with user consent screens
- Client Credentials grant (machine-to-machine) isn't supported
- Known bugs with OAuth in Claude Desktop as of late 2025

**The Pivot: Simple Token Authentication**

Decided to simplify to a query parameter token approach:

1. **Security Trade-off**: Query params in HTTPS are encrypted in transit, but may appear in logs. Acceptable for personal vaults.
2. **Dual Support**: Accept token via query param OR Authorization header for flexibility
3. **UI Integration**: Token shown in the MCP URL for easy copy-paste

### Technical Achievements

**Token Validation Logic:**

```typescript
private validateAuth(req: IncomingMessage): boolean {
  const expectedToken = this.settings.authToken;

  // Check query parameter
  const url = new URL(req.url || '', `http://localhost:${port}`);
  const queryToken = url.searchParams.get('token');
  if (queryToken === expectedToken) return true;

  // Check Authorization header
  const authHeader = req.headers['authorization'];
  if (authHeader?.split(' ')[1] === expectedToken) return true;

  return false;
}
```

**Settings UI Changes:**

- Moved "Require Authentication" toggle under Remote Access section
- Only shows when tunnel is enabled (auth without tunnel is pointless)
- Token field with regenerate button appears when auth is enabled
- MCP URL automatically includes `?token=xxx` when both tunnel and auth are enabled

### Key Learnings

1. **Claude OAuth is complex**: Claude Desktop expects full OAuth 2.1 with PKCE, not simple client credentials
2. **Simple beats complex**: For personal use, a query parameter token is perfectly adequate
3. **HTTPS protects query params**: Query strings are encrypted in transit, only visible in logs
4. **UI flow matters**: Showing the complete URL with token makes copy-paste seamless

### Statistics

- **~100 lines removed** (OAuth complexity)
- **~50 lines added** (simple token auth)
- **1 new setting** (moved authToken under Remote Access)
- **Net simplification**: Fewer endpoints, cleaner code

### Next Steps

- WhatsApp/Telegram bot integration
- Permanent URLs via Named Tunnel (documentation)
- Consider rate limiting for public exposure

---

## 2026-02-01 - GitHub Repository & First Release

**Objective**: Publish the Witness plugin to GitHub and create the first release for BRAT installation.

### What We Built

Made Witness publicly available on GitHub with a proper release for easy installation.

**Repository**: [github.com/jawache/witness](https://github.com/jawache/witness)

**Release**: v0.1.0 - Initial Release

### Release Contents

The v0.1.0 release includes:

- `main.js` - Compiled plugin code (~2MB)
- `manifest.json` - Plugin metadata

**Features in this release:**

- 9 MCP tools (read_file, write_file, list_files, edit_file, search, find_files, move_file, execute_command, get_vault_context)
- Cloudflare Quick Tunnel for remote access
- Token authentication for secure remote connections
- Claude Desktop integration via mcp-remote bridge
- File-based logging
- Full settings UI

### Installation via BRAT

Users can now install Witness easily using the BRAT plugin:

1. Install BRAT from Community Plugins
2. Add Beta Plugin: `jawache/witness`
3. Enable Witness in Community Plugins

BRAT handles downloading the release assets and installing them in the correct location.

### Documentation Updates

Updated README.md with:

- BRAT installation instructions (recommended method)
- Manual installation from releases
- Build from source instructions
- Clear separation of installation methods

### Statistics

- **1 GitHub repository** created
- **1 release** (v0.1.0) published
- **2 release assets** (main.js, manifest.json)
- **README.md** updated with new installation options

### Next Steps

- Submit to Obsidian Community Plugins (requires review)
- WhatsApp/Telegram bot integration
- Permanent URLs via Named Tunnel

---

## 2026-02-01 - Semantic Search Implementation (PAUSED)

**Objective**: Add semantic search capability that finds files by meaning, not just keywords, leveraging Smart Connections embeddings.

**Status**: ⏸️ PAUSED - ONNX WASM runtime doesn't initialize properly in Obsidian's Electron environment. Code preserved in `feature/semantic-search` branch.

### What We Built

Implemented a `semantic_search` MCP tool that:
- Generates query embeddings using transformers.js with `TaylorAI/bge-micro-v2` model
- Loads pre-computed embeddings from Smart Connections' `.smart-env/multi/*.ajson` files
- Calculates cosine similarity between query and document vectors
- Returns ranked list of semantically similar files

### The Journey

**Research Phase: Smart Connections Integration**

Initially explored calling Smart Connections APIs directly, but discovered:
- Obsidian commands are fire-and-forget (no return values)
- Smart Connections doesn't expose a programmatic search API
- Other MCP servers that integrate with SC all read the `.ajson` files directly

This led to the decision to bundle transformers.js and generate query embeddings ourselves.

**The Build Challenge: Native ONNX Bindings**

First attempt with `@xenova/transformers` failed to build:
- ES2018 target doesn't support BigInt literals (used by ONNX)
- Native `.node` files can't be bundled by esbuild
- Same issues with `@huggingface/transformers`

**Solution: Browser Platform + WASM**

Updated esbuild configuration:
- Changed `platform` from "node" to "browser"
- Upgraded `target` to "es2020" for BigInt support
- Added `conditions: ["browser", "import"]` for WASM version
- Aliased `onnxruntime-node` to `onnxruntime-web`
- Marked native modules as external

This forces transformers.js to use the WebAssembly backend instead of native ONNX bindings, which works in Obsidian's Electron environment.

### Implementation Details

**Helper Methods:**
- `initEmbedder()` - Lazy-loads transformers.js pipeline on first use
- `cosineSimilarity()` - Vector similarity calculation
- `loadSmartConnectionsEmbeddings()` - Parses SC's `.ajson` format with caching
- `clearEmbeddingsCache()` - Manual cache invalidation if needed

**The semantic_search Tool:**
```typescript
semantic_search(query: string, limit?: number)
// Returns: List of files ranked by semantic similarity
// Example: "productivity systems" → finds notes about GTD, time management, etc.
```

### Technical Achievements

- Successfully bundled transformers.js (4.2MB) for Obsidian
- Compatible with Smart Connections' embedding format
- Lazy loading prevents startup delay
- Embeddings cached for fast subsequent searches

### Key Learnings

1. **Platform matters**: esbuild's `platform: "browser"` + conditions can force WASM builds
2. **ES target versions**: BigInt requires ES2020 minimum
3. **Smart Connections format**: AJSON files store embeddings keyed by `TaylorAI/bge-micro-v2`
4. **Obsidian = Electron**: Browser APIs available, but Node.js require() still works

### Statistics

- **1 new MCP tool**: semantic_search
- **4 helper methods** added
- **Bundle size**: 4.2MB (increased from transformers.js)
- **10 total MCP tools** now available

### Why Paused

Despite successful bundling, the ONNX WASM runtime fails to initialize in Obsidian's Electron environment:

```text
TypeError: Cannot read properties of undefined (reading 'create')
at createInferenceSession
```

The model downloads correctly but `InferenceSession.create()` fails. The WASM files aren't loading properly in Electron's sandboxed environment.

### Code Preserved

All semantic search code is in the `feature/semantic-search` branch for future work when we find a solution.

### Potential Future Approaches

- Run embeddings in a separate Node.js process
- Use a different model format (GGML/llama.cpp)
- Wait for better Electron/WASM compatibility
- External embedding service

---

## 2026-02-03 - Semantic Search WASM Deep Dive

**Objective**: Get WASM-based embeddings working in Obsidian for semantic search

### What We Tried

Continued investigation into running transformers.js/ONNX in Obsidian's Electron environment. Systematically tested multiple approaches:

1. **Web Worker with bundled transformers.js**
   - Model downloads successfully (0-100%)
   - Fails at ONNX session creation: "no available backend found"

2. **Direct main thread with bundled transformers.js**
   - TypeScript errors with fileURLToPath resolution
   - Electron's hybrid Node.js/browser environment confuses module resolution

3. **Dynamic import from CDN**
   - `import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2')`
   - Works in Electron's renderer process
   - Model downloads successfully
   - Same ONNX session creation failure: "Cannot read properties of undefined (reading 'create')"

4. **Preloading onnxruntime-web separately**
   - Load onnxruntime-web via script tag before transformers.js
   - Set WASM paths explicitly
   - Still fails at InferenceSession.create()

5. **Various env configurations**
   - `env.allowLocalModels = false`
   - `env.useBrowserCache = true`
   - `env.backends.onnx.wasm.wasmPaths = CDN_URL`
   - None resolved the core issue

### Root Cause Analysis

The fundamental problem is that **Obsidian's Electron renderer process is a hybrid Node.js/browser environment** that confuses ONNX runtime's backend selection:

1. ONNX detects Node.js APIs are available → tries Node backend
2. Node backend requires native binaries → fails
3. Falls back to WASM backend
4. WASM backend can't initialize properly because of hybrid environment
5. `InferenceSession.create` becomes undefined

This is a **known limitation** with transformers.js in Electron environments. The library is designed for either pure Node.js OR pure browser, not Electron's hybrid.

### THE SOLUTION: Iframe Isolation

After examining the Smart Connections plugin codebase (which does WASM embeddings successfully), found the key insight: **use a hidden iframe to provide a clean browser context**.

**How it works:**
1. Create a hidden iframe with `srcdoc` containing a module script
2. The iframe runs in a pure browser context without Node.js APIs
3. Load transformers.js from CDN inside the iframe
4. Communicate via `postMessage` between main plugin and iframe
5. WASM initializes successfully in the clean browser environment!

**Implementation:**
```typescript
// Create hidden iframe
this.iframe = document.createElement('iframe');
this.iframe.style.display = 'none';
this.iframe.srcdoc = `
  <script type="module">
    const transformers = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.8.0');
    // ... pipeline initialization works here!
  </script>
`;
document.body.appendChild(this.iframe);
```

**Results:**
- ✅ Model downloads successfully
- ✅ WASM/ONNX runtime initializes
- ✅ Embeddings generated (384 dimensions)
- ✅ Sub-second response time after initial load

### Code Changes

- Created `embedding-service-iframe.ts` with iframe-based approach
- Uses `@huggingface/transformers@3.8.0` from CDN
- Tries WebGPU first, falls back to WASM
- Updated `main.ts` to use `EmbeddingServiceIframe`

### Key Learning

**The iframe isolation pattern is the solution for running WASM in Obsidian plugins.** The iframe provides a clean browser environment where:
- No Node.js APIs pollute the global scope
- Module resolution follows browser rules
- WASM backend initialization works correctly

Credit to Smart Connections for pioneering this approach!

### Next Steps

1. ✅ WASM embeddings working via iframe
2. Phase B: Implement storage & indexing in .witness/
3. Phase C: Complete semantic_search tool with actual document search
4. Phase D: Settings UI for semantic search
5. Phase E: Obsidian Leaf View UI

---

## 2026-02-04 - Semantic Search Complete: Storage, Indexing & Search

**Objective**: Complete the semantic search implementation with document indexing, storage, and actual search functionality.

### What We Built

Completed the full semantic search pipeline:

1. **EmbeddingIndex** (`src/embedding-index.ts`)
   - Storage in `.witness/embeddings/` folder
   - Index metadata tracking model, version, document count
   - Per-file vector storage in `vectors/` subdirectory
   - Cosine similarity search with filtering by tags/paths
   - Incremental update support

2. **DocumentIndexer** (`src/document-indexer.ts`)
   - Hierarchical chunking: document-level + H2 section embeddings
   - Front matter extraction (tags, title)
   - Content hashing for change detection
   - Progress callbacks for UI feedback

3. **MCP Tools**
   - `semantic_search` - Find documents by meaning with filters
   - `index_documents` - Build/update the embedding index

4. **Incremental Updates**
   - File change listeners (`modify`, `delete`, `rename`)
   - Automatic re-indexing on file changes
   - Skip unchanged files during batch indexing

### The Journey

**Challenge: Wrong Test Vault**

Spent significant time debugging why `index_documents` tool wasn't appearing. Discovered there were TWO vaults:
- `test-vault/` - where we were copying files
- `test/vault/` - the actual vault open in Obsidian

After copying to the correct location and restarting Obsidian, everything worked immediately.

**Testing the Full Pipeline:**

```bash
# Initialize session
curl -X POST http://localhost:3001/mcp ...

# Index documents - SUCCESS!
{"text":"Indexing complete!\n\n- Indexed: 8 documents\n- Skipped (unchanged): 0\n- Errors: 0\n\nModel: TaylorAI/bge-micro-v2 (384 dimensions)"}

# Search by meaning - SUCCESS!
{"text":"Found 9 result(s) for: \"chaos and order philosophy\"\n\n1. **move-conflict-source** (50.5%)\n..."}
```

The search returns both document-level and section-level results with similarity percentages.

### Achievements

- **Iframe Pattern Works**: transformers.js runs successfully in hidden iframe
- **384-dimension embeddings**: Using TaylorAI/bge-micro-v2 model
- **Sub-second queries**: After initial model load, search is fast
- **Hierarchical results**: Section headings with line numbers returned
- **Filter support**: Can filter by tags and folder paths

### Learnings

1. **Always verify the correct vault**: Development vs test vaults can cause confusion
2. **Obsidian requires full restart**: Copying files isn't enough - JS must be reloaded
3. **Iframe isolation is the key**: Clean browser context for WASM initialization
4. **postMessage for communication**: Simple and reliable across iframe boundary

### Session Stats

- **3 new source files** created
- **~600 lines** of new TypeScript
- **2 new MCP tools** (semantic_search, index_documents)
- **14 total MCP tools** now available
- **8 documents indexed** in test vault

### Files Changed

- `src/embedding-service-iframe.ts` - Iframe WASM embeddings (created earlier)
- `src/embedding-index.ts` - NEW: Storage and search
- `src/document-indexer.ts` - NEW: Document processing
- `src/main.ts` - Updated with new tools and file listeners
- `README.md` - Updated feature list and roadmap
- `CLAUDE.md` - Updated phase status and documentation

### What's Next

- Phase D: Settings UI for semantic search (exclude paths, index status)
- Phase E: Obsidian Leaf View for visual search interface
- Consider: Background indexing on vault open

---

*End of log entry*

## 2026-02-04 - Smart Connections Integration

**Objective**: Replace custom iframe-based embedding indexing with Smart Connections' pre-built embeddings for semantic search.

### The Problem

The custom embedding approach (iframe + transformers.js + WASM) worked but had stability issues at scale. When indexing a vault with ~2000 documents, approximately 10% of files (~200) would fail to index due to WASM memory corruption. The iframe reset mechanism helped but didn't fully solve it. Rather than fight WASM instability, we pivoted to reading embeddings from the Smart Connections plugin, which has already solved these problems with its mature indexing system.

### What We Built

- **SmartConnectionsReader** (`src/smart-connections-reader.ts`): New class that reads and caches embeddings from Smart Connections' `.smart-env/multi/*.ajson` files
  - Validates SC is installed with correct model (`TaylorAI/bge-micro-v2`)
  - Parses AJSON format (append-only JSON, one entry per line with trailing commas)
  - Incremental caching using mtime tracking per SC file
  - Cosine similarity search against cached vectors

- **Updated `semantic_search` tool**: Now uses SmartConnectionsReader for document embeddings while keeping iframe embedding service for query embeddings only

- **Removed `index_documents` tool**: SC handles all indexing, so this tool is no longer needed

- **Cleaned up old embedding code**: Removed 5 source files (~1,450 lines) that are no longer needed

### The Journey

**Feature branch preservation**: Before starting, we preserved all the iframe embedding work on the `feature/iframe-embeddings` branch in case we ever need it again.

**SC file format discovery**: Smart Connections stores embeddings in `.smart-env/multi/*.ajson` files. The AJSON format is append-only JSON with one entry per line. Each line looks like:
```
"smart_sources:path/to/file.md": {"path":"path/to/file.md","embeddings":{"TaylorAI/bge-micro-v2":{"vec":[...384 floats...]}},...},
```
Key gotcha: lines end with a trailing comma which must be stripped before JSON parsing.

**mtime caching bug**: The first implementation tried to map SC filenames back to vault paths for cache lookup (SC uses `!` for hidden paths like `.obsidian/`, while our conversion used `_`). This meant cache lookups failed and every search re-read all 4000+ files from disk (~7 seconds). Fixed by tracking mtimes directly keyed by SC file path, with a separate `scFileToVaultPath` map. After fix: cached searches complete in ~265ms.

**Deleted files cleanup**: Also removed the esbuild worker configuration since we no longer build a separate Web Worker bundle.

### Technical Details

**Architecture**:
```
semantic_search request
    → SmartConnectionsReader.validate() - check SC config & model
    → SmartConnectionsReader.loadEmbeddings() - incremental, mtime-based
    → EmbeddingServiceIframe.embed(query) - WASM query embedding
    → SmartConnectionsReader.search() - cosine similarity
    → Ranked results
```

**Performance on main vault (4,097 documents)**:
- First search: ~14s (load all embeddings from disk + model init)
- Subsequent searches: ~265ms (mtime checks + query embedding + search)
- Memory: ~7MB for 4097 cached vectors (384 floats × 4 bytes × 4097)
- Search execution: <10ms for cosine similarity against all vectors

**Files removed** (1,451 lines deleted):
- `src/embedding-index.ts` (418 lines) - Custom storage
- `src/document-indexer.ts` (428 lines) - Custom indexing
- `src/embedding-service.ts` (232 lines) - Web Worker approach
- `src/embedding-service-direct.ts` (227 lines) - Direct approach
- `src/embedding-worker.ts` (146 lines) - Worker file

**Files kept**:
- `src/embedding-service-iframe.ts` - Still used for query embedding generation

### Key Learnings

1. **Leverage existing plugins**: Smart Connections has years of development solving WASM/ONNX stability issues. Reading their embeddings is far more reliable than reimplementing indexing.
2. **AJSON trailing commas**: SC's append-only JSON format has trailing commas on each line - must strip before parsing.
3. **SC filename conventions**: Files starting with `.` get `!` prefix in SC filenames (e.g., `.obsidian/` → `!obsidian_`). Don't try to reverse-engineer this; track by SC file path directly.
4. **mtime-based caching**: Track mtimes keyed by the actual file path you're checking (SC file path), not by a derived/converted path. Simple and reliable.

### Session Stats

- **1 new source file** created (`src/smart-connections-reader.ts`, ~360 lines)
- **5 source files** deleted (~1,450 lines)
- **Net: ~1,090 lines removed** while adding the same functionality
- **13 MCP tools** (down from 14 - removed `index_documents`)
- **4,097 documents** successfully searched in main vault

### Files Changed

- `src/smart-connections-reader.ts` - NEW: SC file reading and caching
- `src/main.ts` - Updated semantic_search, removed index_documents
- `esbuild.config.mjs` - Removed worker build configuration
- `docs/features/smart-connections-integration.md` - NEW: Feature specification
- `src/embedding-index.ts` - DELETED
- `src/document-indexer.ts` - DELETED
- `src/embedding-service.ts` - DELETED
- `src/embedding-service-direct.ts` - DELETED
- `src/embedding-worker.ts` - DELETED

### What's Next

- Fix first-search latency (~14s for 4000 docs) - consider background preloading on plugin start
- Settings UI for semantic search (exclude paths, index status)
- Obsidian Leaf View for visual search interface

---

*End of log entry*

## 2026-02-05 - Named Tunnel & Primary Machine

**Objective**: Replace ephemeral Quick Tunnel URLs with permanent Named Tunnel support, and add a Primary Machine feature for multi-device Obsidian Sync setups.

### What We Built

Two major features for the remote access system:

1. **Cloudflare Named Tunnel Support** - Permanent URLs using your own domain
2. **Primary Machine Designation** - Only one machine runs the tunnel when syncing across devices

### The Journey

**Named Tunnel Implementation**

The user was tired of Quick Tunnel URLs changing every time Obsidian restarts. They had already created a Named Tunnel in the Cloudflare dashboard pointing `witness.asim.dev` to `localhost:3456`.

The `cloudflared` npm package provides two APIs:
- `Tunnel.quick(url)` - Quick tunnel with random trycloudflare.com URL
- `Tunnel.withToken(token)` - Named tunnel using pre-configured dashboard settings

Key discovery: Named tunnels do NOT emit a `url` event. That event is specifically for trycloudflare.com URLs where the URL isn't known in advance. Named tunnels emit a `connected` event instead, since the URL is already configured in the Cloudflare dashboard. This required listening for `connected` instead of `url` to update tunnel status.

**Settings UI Updates:**

- Added tunnel type dropdown: Quick vs Named
- Added password-masked token field (300px wide for long JWT tokens)
- Added editable tunnel URL field for the user's public hostname
- MCP URL display automatically constructs the full URL with /mcp path and ?token= parameter

**The Port Mismatch Discovery**

Initial testing failed because the Cloudflare tunnel was configured for `localhost:3456` but the plugin was running on port 3000. Changed the port in settings to match.

**The /health 404 Mystery**

After connecting the tunnel, `/health` returned 404 while `/mcp` returned 401 (correct auth error). The cause: the Cloudflare tunnel ingress rules were path-restricted to `/mcp` only. After the user removed the path restriction in the dashboard, all endpoints worked through the tunnel.

**Orphaned Process Cleanup**

Discovered 17 orphaned `cloudflared tunnel --url http://localhost:3000` processes from previous Quick Tunnel sessions that weren't properly cleaned up on Obsidian restart. Killed them all.

**Multi-Device Problem**

The user asked: "What happens if multiple machines with Obsidian Sync all have the same tunnel token?" Answer: Cloudflare treats them as multiple connectors for the same tunnel and round-robins traffic between them. Since each machine has a different vault state, this produces unpredictable results.

**Primary Machine Feature**

Solution: Added `tunnelPrimaryHost` setting that stores `os.hostname()` of the designated machine. At the top of `startTunnel()`, the plugin checks if the current hostname matches. If not, it silently skips starting the tunnel.

Settings UI for primary machine:
- Shows current host vs configured host
- "Set as primary" button (disabled if already primary)
- "x" button to clear (allow all machines)
- Warning color when current machine is NOT the primary

**Mobile Considerations**

Obsidian mobile (iOS/Android) cannot run the HTTP server or cloudflared binary. The tunnel and MCP server features are desktop-only. Mobile devices access the vault as clients through the tunnel URL.

### Technical Details

**New Settings:**
```typescript
tunnelType: 'quick' | 'named';   // Tunnel mode
tunnelToken: string;              // Cloudflare tunnel JWT token
tunnelPrimaryHost: string;        // Hostname of primary machine
```

**Named Tunnel Connection:**
```typescript
if (isNamed) {
    this.tunnelProcess = Tunnel.withToken(this.settings.tunnelToken);
    this.tunnelProcess.once('connected', async (connection) => {
        this.tunnelStatus = 'connected';
        // URL comes from user settings, not from event
    });
}
```

**Primary Host Check:**
```typescript
const currentHost = os.hostname();
if (this.settings.tunnelPrimaryHost &&
    this.settings.tunnelPrimaryHost !== currentHost) {
    this.logger.info(`Tunnel skipped: not primary host`);
    return;
}
```

### Key Learnings

1. **Named tunnels don't emit `url` events**: Only Quick Tunnels (trycloudflare.com) emit the `url` event. Named tunnels emit `connected` with location/IP info.
2. **Cloudflare ingress rules matter**: Path restrictions in the tunnel config affect which endpoints are accessible. Keep it open or match all your endpoints.
3. **Round-robin is the default**: Multiple cloudflared connectors on the same tunnel = load balancing. Not what you want for personal vaults.
4. **`os.hostname()` for machine identity**: Simple and reliable way to identify which machine is which across Obsidian Sync.
5. **Orphan processes accumulate**: Quick Tunnels from previous sessions may not be cleaned up on crash/force-quit. Consider periodic cleanup.

### README Overhaul

Rewrote the entire Remote Access section with:
- Option A: Quick Tunnel (easy, temporary)
- Option B: Named Tunnel with 3-step Cloudflare dashboard guide
- Multiple Devices section with Primary Machine explanation
- Mobile Devices section explaining desktop-only limitations
- Security Notes section

### Statistics

- **3 new settings** added to WitnessSettings
- **~60 lines** of tunnel code changes
- **~80 lines** of settings UI additions
- **README** Remote Access section completely rewritten
- **17 orphaned processes** cleaned up
- **1 successful end-to-end test** through `witness.asim.dev`

### Files Changed

- `src/main.ts` - Named tunnel support, primary host check, settings UI
- `README.md` - Comprehensive remote access documentation
- `CLAUDE.md` - Updated with session learnings
- `manifest.json` - Version bump to 0.4.0
- `package.json` - Version bump to 0.4.0

### What's Next

- WhatsApp/Telegram bot integration (Phase 2 remaining)
- Phase 4: Heartbeat scheduler, chaos monitoring
- Consider: automatic orphan process cleanup on startup

---

*End of log entry*

## 2026-02-05 - Dataview Integration & Orientation Consolidation

**Objective**: Integrate the Dataview plugin's query engine into Witness MCP tools, consolidate the main vault's orientation files into a single comprehensive document, and refine the knowledge-saving workflow.

### What We Built

1. **`dataview_query` MCP tool** — Execute DQL queries directly from an AI client
   - Supports markdown and JSON output formats
   - Structured data: `dvApi.query()` with Link-to-string conversion
   - Rendered tables: `dvApi.queryMarkdown()` for human-readable output
   - Always registered at startup; checks Dataview availability at call time

2. **`read_file` render parameter** — Optional `render: true` flag that resolves Dataview codeblocks before returning content

3. **`get_orientation` auto-rendering** — Orientation document always has its Dataview queries resolved, so the AI sees live data (prompt tables, template lists) instead of raw codeblocks

4. **Consolidated ORIENTATION.md** — Merged three separate context files in the main vault into one comprehensive document covering vault philosophy, knowledge collections, templates, prompts, heartbeat, writing style, and the full knowledge-saving workflow

5. **Knowledge-saving workflow** — Detailed "Saving Knowledge to the Vault" section with:
   - Existence checking (text + semantic search before creating)
   - Citation-by-default policy (footnotes for external sources without being asked)
   - Internal wiki-links via semantic search (knowledge-to-knowledge only)
   - Frontmatter references for provenance
   - Summary table of all 5 link types

6. **Session skills** — Updated `/wrap-up` to include git push, created `/release` for GitHub releases with version bumps

### The Journey

The session started with Dataview research — confirming that the `dvApi.queryMarkdown()` and `dvApi.query()` APIs were accessible from within Obsidian's plugin context. The key architectural insight was that Witness has a unique advantage: it runs *inside* Obsidian, so it can call Dataview's API directly rather than scraping rendered HTML.

The first implementation hit a plugin load-order bug: Dataview hadn't initialised yet when Witness registered its tools at startup. The conditional `if (this.isDataviewAvailable())` check silently skipped registration entirely. The fix was straightforward — always register the tool, check availability at call time, and return a helpful error if Dataview isn't loaded.

The orientation consolidation was more involved. Three separate files (root ORIENTATION.md, knowledge ORIENTATION.md, and relevant CLAUDE.md sections) were merged into a single document. Multiple rounds of refinement followed — removing sections that belong in system prompts, expanding the citation and linking rules, and establishing the "citations by default" policy.

### Technical Achievements

- **15 MCP tools** now available (was 13)
- **31 integration tests** passing (8 new Dataview tests)
- Regex-based Dataview codeblock detection: `/```dataview\n([\s\S]*?)```/g`
- Link object serialisation for JSON format output
- Test vault now includes Dataview plugin and 3 topic files for testing

### Key Learnings

- **Plugin load order matters**: Always register tools unconditionally and check runtime availability in the handler. Obsidian doesn't guarantee plugin load order.
- **Dataview API is clean**: `queryMarkdown()` returns `{ successful: boolean, value: string, error?: string }` — no parsing needed.
- **Orientation files benefit from live data**: Having Dataview queries in the orientation document means the AI always sees current prompt and template lists without manual updates.

### Statistics

- Files changed: 5 modified, 6 new
- Lines added: ~350
- New MCP tools: 1 (`dataview_query`)
- New tool parameters: 1 (`read_file.render`)
- New integration tests: 8
- Total integration tests: 31

### What's Next

- Semantic search feature implementation (Phase A-E from feature spec)
- WhatsApp/Telegram bot integration
- Phase 4: Heartbeat scheduler, chaos monitoring

---

*End of log entry*

## 2026-02-05 - Claude Code Configuration & Desktop-Only Flag

**Objective**: Set up reusable Claude Code configuration for use across projects, and mark the plugin as desktop-only.

### What We Built

- **User-level Claude Code skills** (`~/.claude/skills/`):
  - `/wrap-up` — session wrap-up protocol (DEVLOG, docs, commit, push)
  - `/release` — version bump, build, GitHub release creation
  - `/init-project` — scaffold standard project files (CLAUDE.md, DEVLOG, README.md, TODO.md, docs/)
- **User-level CLAUDE.md** (`~/.claude/CLAUDE.md`) — shared conventions: project structure, DEVLOG format, TODO.md usage, feature file specs, commit style, British English preference
- **User-level settings** (`~/.claude/settings.json`) — permissive permissions (Bash(*), Read, Edit, Write, WebSearch, WebFetch) so every project gets them by default
- **Desktop-only flag** — set `isDesktopOnly: true` in manifest.json since the plugin runs an HTTP server and cloudflared binary

### The Journey

Started with the question of how to share Claude Code configuration across projects. Discovered the configuration hierarchy: user-level (`~/.claude/`) vs project-level (`.claude/`). Converted the existing JSON skill files to the preferred markdown SKILL.md format with YAML frontmatter, generalised them so they're not Witness-specific, and placed them at user level.

Also captured the user's project conventions: TODO.md linking to feature specs in `docs/features/`, `docs/spec.md` for original specifications, and the pattern of ending feature discussions with a spec file.

### Key Learnings

- Claude Code skills live in `~/.claude/skills/<name>/SKILL.md` for user-level (all projects) or `.claude/skills/<name>/SKILL.md` for project-level
- Both `.claude/commands/` (legacy) and `.claude/skills/` work, but skills is the current standard
- Skills use markdown with YAML frontmatter, not JSON — `allowed-tools` in frontmatter grants auto-approved permissions
- User-level `~/.claude/CLAUDE.md` applies to all projects — good for shared conventions
- User-level `~/.claude/settings.json` applies globally — no need for per-project settings files if permissions are the same

### Statistics

- 5 files created at user level (~/.claude/)
- 1 file modified in repo (manifest.json)
- Skills converted from JSON to markdown format

### Next Steps

- Test `/init-project` in a new project directory
- Test `/wrap-up` and `/release` in a fresh session
- Consider adding more shared skills as patterns emerge

---

*End of log entry*

## 2026-02-05 - Ollama + Orama: Own the Embedding Index

**Objective**: Replace Smart Connections + iframe WASM embedding approach with Ollama for embedding generation and Orama for vector storage. Own the entire embedding index.

### What We Built

1. **OllamaProvider** (`src/ollama-provider.ts`, ~110 lines) — HTTP client for Ollama's `/api/embed` endpoint
   - Batch embedding support (multiple texts in one call)
   - Model availability checking (`/api/tags`)
   - Dimension lookup for known models
   - Default: `nomic-embed-text` (768 dimensions, 274MB)

2. **VectorStore** (`src/vector-store.ts`, ~250 lines) — Orama-backed vector database
   - Schema: path, title, mtime, embedding vector
   - Single-file persistence at `.witness/index.orama`
   - Incremental indexing: only re-embeds files with changed mtime
   - Batch indexing with progress callbacks
   - Cosine similarity search with path filtering

3. **Updated `semantic_search` tool** — Lazy-initialises Ollama + Orama on first use
   - Checks Ollama availability, returns helpful error if not running
   - Auto-indexes stale files before searching
   - Persists index to disk after indexing

4. **Removed Smart Connections integration** — Deleted `src/smart-connections-reader.ts` (343 lines) and `src/embedding-service-iframe.ts` (540 lines). Net deletion of ~883 lines of fragile code.

### The Journey

The session started with revisiting the embedding strategy. The iframe WASM approach worked but was brittle — ~400 lines of code handling WebGPU detection, fp16/fp32 fallback, WASM fallback, throttling, and iframe lifecycle. Smart Connections dependency tied us to another plugin's proprietary `.ajson` format, which isn't sustainable for a public plugin.

**Ollama as the answer**: Since the user already runs Ollama on the same machine, we can replace both SC reading AND iframe WASM with simple HTTP calls to `localhost:11434/api/embed`. No WASM, no iframe, no proprietary formats.

**Storage: Orama over individual JSON files**: The earlier custom embedding index used one JSON file per document, causing multi-second load times on large vaults. Orama provides a pure JS embedded search engine (<2kb bundled) that serialises the entire database to a single JSON file. Load once, search in microseconds.

**The implementation went smoothly**: Created `OllamaProvider` and `VectorStore` as clean, focused classes. Updated `src/main.ts` to use them instead of SC/iframe. Built successfully after fixing Orama's internal type casting (needed `as unknown as VaultDocument`).

**End-to-end test**: Launched Obsidian with the test vault, initialised an MCP session, and called `semantic_search` with query "test vault integration". Got 5 results back:

1. README.md (66.9%)
2. subfolder/nested.md (43.0%)
3. test-read.md (40.6%)
4. topics/carbon-accounting.md (38.4%)
5. dataview-test.md (38.1%)

### Key Learnings

1. **Simple HTTP > iframe WASM**: Replacing ~540 lines of iframe/WASM/WebGPU code with a 20-line HTTP fetch to Ollama. The complexity difference is staggering.
2. **Orama is excellent**: Pure JS, zero dependencies, vector + full-text support, single-file serialisation. Perfect for Obsidian plugins.
3. **Own your index**: Depending on another plugin's internal data format is fragile. Generating and storing embeddings ourselves gives full control.
4. **Ollama batch API**: The `/api/embed` endpoint accepts an array of inputs, reducing round-trips for bulk indexing.

### Statistics

- **2 new source files** created (~360 lines)
- **2 source files** deleted (~883 lines)
- **Net: ~520 lines removed** while keeping the same functionality
- **1 npm package** added (`@orama/orama`)
- **Bundle size**: 2.2MB (down from previous builds with transformers.js overhead)
- **5 test results** returned successfully from end-to-end test

### Files Changed

- `src/ollama-provider.ts` — NEW: Ollama HTTP client
- `src/vector-store.ts` — NEW: Orama vector store
- `src/main.ts` — Updated imports, instance vars, semantic_search handler
- `src/smart-connections-reader.ts` — DELETED
- `src/embedding-service-iframe.ts` — DELETED
- `docs/features/ollama-integration.md` — NEW: Feature specification
- `TODO.md` — Added Ollama integration task
- `package.json` — Added @orama/orama dependency
- `manifest.json` — isDesktopOnly: true (both demo + test vaults)

### What's Next

- Integration tests for the new Ollama/Orama semantic search
- Settings UI for Ollama configuration (base URL, model selection)
- Background indexing on vault open
- Progress indicator during initial index build

---

*End of log entry*

## 2026-02-07 - Fixing Embedding Quality: Context Limits, Task Prefixes, and Short Document Noise

**Objective**: Debug and fix the 565 embedding failures with nomic-embed-text, then systematically fix search quality issues uncovered during real-world testing on a 4,097-document vault.

### What We Built

- Dynamic model info resolution via Ollama's `/api/show` endpoint
- Model-specific task prefixes for nomic-embed-text and mxbai-embed-large
- Minimum content length filter (configurable, default 50 chars) to suppress short document noise
- Race condition fixes in the settings UI async lifecycle
- Fixed Clear Index to work even when VectorStore hasn't been loaded
- Comprehensive documentation of embedding model behaviour and gotchas

### The Journey

This session was a deep debugging journey into why semantic search was returning poor results. It started with a simple symptom — 565 out of ~4,000 files failing to embed — and ended with a thorough understanding of how embedding models actually work under the hood.

**Chapter 1: The 565 Failures**

After switching from mxbai-embed-large to nomic-embed-text, 565 files consistently failed with Ollama returning 400 "index length exceeded context length". We had already added `truncate: true` and client-side pre-truncation in the previous session, so why were files still failing?

The breakthrough came from querying Ollama's `/api/show` endpoint directly:

```bash
curl -s http://localhost:11434/api/show -d '{"model":"nomic-embed-text"}' | jq '.model_info'
```

This revealed `nomic-bert.context_length: 2048` — but our hardcoded `MODEL_CONTEXT_TOKENS` had `8192` for nomic-embed-text, taken from the Modelfile's `num_ctx` parameter. These are fundamentally different things: `num_ctx` is a runtime parameter for generative models, but BERT-based embedding models have a fixed architecture context window baked into their weights. For nomic-embed-text, that's 2048 tokens, not 8192.

With `maxChars = 8192 * 2 = 16384`, we were sending documents up to 16K characters — far exceeding the actual 4096-character limit (2048 tokens * 2 chars/token). Fixing the constant to 2048 immediately resolved all 565 failures.

**Chapter 2: Dynamic Resolution**

Hardcoded constants are fragile. The user rightly pointed out that if Ollama's `/api/show` provides the real values, we should use them. We added `resolveModelInfo()` to OllamaProvider, which queries `/api/show` once at startup and caches both `embedding_length` (dimensions) and `context_length` (context window). The hardcoded maps now serve as fallbacks when Ollama is unreachable.

**Chapter 3: Clear Index Doesn't Actually Clear**

Testing revealed that "Clear Index" didn't work after a plugin restart. The bug: `clearIndex()` checked `if (this.vectorStore)` — but after a restart, `vectorStore` is null (it's lazily initialised on first search). So the method did nothing, leaving the old `.witness/index.orama` file on disk. Next search would load the old index.

The fix was twofold: if `vectorStore` is null, delete the file directly from disk. And after clearing, set `vectorStore` to null to ensure a fresh one is created next time.

**Chapter 4: Vector Search Returns Junk**

After re-indexing with the corrected context length, vector search still returned completely irrelevant results. Searching for "water" returned an empty document called "mica app" (content: just the word "gold"). Fulltext search worked fine, so the issue was specifically with vector similarity.

The root cause: **nomic-embed-text requires task prefixes**. Documents must be prefixed with `search_document: ` and queries with `search_query: `. Without these, both documents and queries embed into a generic space where similarity scores are meaningless. This is documented in nomic's model card but easy to miss.

We added `MODEL_TASK_PREFIXES`, `embedDocuments()`, and `embedQuery()` to OllamaProvider. The VectorStore was updated to use these instead of raw `embed()`.

**Chapter 5: Which Other Models Need Prefixes?**

This discovery raised an important question: do our other supported models also need prefixes? We researched each one systematically:

- **nomic-embed-text**: Both prefixes mandatory (search_document / search_query)
- **mxbai-embed-large**: Query prefix only ("Represent this sentence for searching relevant passages: ")
- **all-minilm**: No prefix needed
- **bge-m3**: No prefix needed
- **bge-large**: Optional query prefix (marginal improvement)
- **snowflake-arctic-embed**: Query prefix needed

We added mxbai-embed-large to the prefix map (the user's preferred model), with an empty document prefix and the full query prefix.

**Chapter 6: The Null Save Race Condition**

During indexing, the user hit "cannot read properties of null reading save". This was a race condition: `loadIndexCount()` (called when the settings tab opens) runs async and creates a VectorStore. If the user clicks "Build Index" before it completes, both paths create VectorStores, and one might go null before the other finishes.

Fix: the Build Index handler now captures a local `const vs = this.plugin.vectorStore` reference before the async loop. Even if `clearIndex()` or `loadIndexCount()` nullifies the plugin-level reference during indexing, the local reference remains valid.

**Chapter 7: Short Document Noise**

Even with prefixes fixed, short documents kept appearing in results. The "mica app" file (content: "gold") matched every query. This is a fundamental property of how embedding models work: short texts lack distinctive features, so their embeddings cluster near the centre of the vector space. Since cosine similarity measures the angle between vectors, these centroid-adjacent embeddings score moderately high against everything.

The solution: a configurable `minContentLength` setting (default 50 characters) that filters files by `file.stat.size` before indexing. This is a simple, effective improvement — the single most impactful setting for search quality besides choosing the right model.

### Technical Achievements

- **Dynamic model introspection**: No more guessing dimensions or context lengths from documentation. Query the model and cache the results.
- **Correct task prefix handling**: Embeddings now land in the right vector space for each model, producing meaningful similarity scores.
- **Race-condition-safe async UI**: Captured local references prevent null pointer errors during long-running operations.
- **Comprehensive model documentation**: Every supported embedding model researched and documented with its prefix requirements, context limits, and characteristics.

### Key Learnings

1. **`num_ctx` != context_length for BERT models**: The Modelfile parameter is for generative models. Embedding models have fixed architecture context windows. Always check the model's actual `context_length` from `/api/show` model_info.

2. **Task prefixes are not optional**: Some embedding models (nomic-embed-text, mxbai-embed-large) require specific prefixes to produce useful embeddings. Without them, documents and queries embed into a generic space where similarity is meaningless. Always check the model card.

3. **Short documents produce noise, not signal**: A file containing "gold" will match any query because it lacks distinctive features. Filtering by minimum content length is the cheapest and most effective quality improvement.

4. **Characters per token varies wildly**: English prose averages ~4 chars/token, but JSON, HTML, URLs, and special syntax can be as low as ~1.5. Use a conservative estimate (we use 2) for pre-truncation.

5. **Race conditions in async UI**: When a settings page loads data asynchronously and users can click buttons during loading, capture local references to mutable state. Don't rely on `this.plugin.vectorStore` remaining non-null across `await` boundaries.

6. **Web research first**: When hitting a bug with an external tool (Ollama, Orama, etc.), always check the API docs and other people's issues before guessing. The `truncate: true` parameter, the `/api/show` endpoint, and the task prefix requirements were all documented — we just needed to find them.

### Statistics

- 3 files changed, 81 insertions, 14 deletions (this session's commit)
- 26 files changed, ~4,700 insertions, ~1,200 deletions (full Ollama integration branch)
- 53 tests (52 passing, 1 pre-existing failure)
- 6 supported embedding models documented with prefix/context requirements
- 0 embedding failures after fixes (was 565)

### Files Changed

- `src/ollama-provider.ts` — Task prefixes, embedDocuments/embedQuery, resolveModelInfo
- `src/vector-store.ts` — Use embedDocuments/embedQuery instead of raw embed
- `src/main.ts` — minContentLength setting, race condition fixes, loadIndexCount, settings UI

### What's Next

- Markdown chunking — split long documents by headings before embedding for better retrieval on long files
- Background indexing — automatic incremental indexing with status bar indicator
- Re-ranking — optional Ollama-based re-ranking for higher precision
- Chaos triage — MCP tools for processing chaos items one at a time

### Reflections

This session was a masterclass in how many layers of subtlety hide behind "just call an embedding API". Context lengths aren't what they seem. Prefixes that look optional are mandatory. Short documents that seem harmless are the worst polluters. Each fix was simple once understood, but finding the root cause required peeling back assumptions layer by layer. The user's instinct to verify each model's documentation before committing to a prefix map was exactly right — different models have genuinely different APIs, and getting them wrong produces silently wrong results rather than obvious errors.

---

*End of log entry*

## 2026-02-08 - Markdown Chunking, Search Panel Polish & Unified Search Architecture

**Objective**: Implement markdown chunking for better retrieval on long documents, polish the search panel UI, and design the next-generation search architecture.

### What We Built

- **Markdown chunker** (`src/chunker.ts`) — Splits documents by H2 boundaries with H3 subdivision and fixed-size fallback for oversized chunks. Each chunk carries its heading path for context.
- **Schema v3 for VectorStore** — Added `sourcePath`, `headingPath`, `chunkIndex` fields. Documents are now multi-chunk with IDs like `filepath#0`, `filepath#1`.
- **Search panel redesign** — New result layout: title + score row, file path, section indicator with § symbol, snippet. Click-to-heading navigation via `openLinkText()`.
- **Frontmatter stripping** — Snippets now strip YAML frontmatter before showing the first 200 chars.
- **MCP response formatting** — `semantic_search` results now include heading path and snippet in the markdown response.
- **Unified search feature spec** (`docs/features/unified-search.md`) — Complete architecture for consolidating three search tools into two, switching BM25 → QPS, and adding tag/path filtering.
- **Chunker test suite** (`test/unit/chunker.test.ts`) — 19 tests covering H2/H3 splitting, frontmatter handling, edge cases.

### The Journey

The session started with implementing the chunker — a satisfying piece of pure logic that splits markdown by heading boundaries. The interesting design decisions were: should H3s subdivide H2 chunks? (Yes, for better retrieval granularity.) What about chunks with no headings? (Use the whole document as one chunk.) What about enormous chunks? (Fixed-size fallback at 5000 chars.)

With chunking working, the search panel needed a complete redesign. The old flat list of results didn't show which section of a document matched. The new layout shows the heading path with a § indicator, and clicking navigates directly to that heading in the document using Obsidian's `openLinkText()` with a heading anchor.

Then came the deeper architectural question: we now have three overlapping search tools (`search`, `semantic_search`, `find_files`). The brute-force `search` tool reads every file and regex-matches line by line — returning 77KB of unranked `file:line - text` noise for common queries. We explored whether Obsidian's built-in search API could replace it, but discovered there's no official headless search API — only low-level `prepareSimpleSearch`/`prepareFuzzySearch` string matchers and an undocumented internal plugin hack.

The breakthrough came from deep-diving into Orama's capabilities:
- **All schema fields are optional at insert time** — documents without embeddings are simply omitted from vector index but remain fully searchable via BM25
- **`enum[]` type** supports `containsAll`/`containsAny` operators — perfect for Obsidian tags
- **`where` filters** support `and`/`or`/`not` logical combinators
- **QPS (Quantum Proximity Scoring)** — a drop-in BM25 replacement that scores based on token proximity, making phrase-like queries ("carbon intensity") rank correctly

This led to the unified search architecture: two tools (`search` for content, `find` for file discovery), a `SearchEngine` abstraction interface, two-phase indexing (content first, embeddings second), and QPS for better text matching.

### Technical Achievements

1. **Heading-aware chunking** with path tracking (e.g., "## Overview > ### Principles")
2. **Multi-chunk deduplication** — search results show best chunk per file, not every chunk
3. **Click-to-heading navigation** in search panel via `openLinkText()` with heading anchors
4. **Complete feature spec** for unified search with MCP tool descriptions, schema design, and implementation plan
5. **67/68 tests passing** across unit and integration suites

### Key Learnings

1. **Orama schema fields are optional at insert time** — this is the key insight that enables indexing all files regardless of embedding success. Just omit the `embedding` field; the document stays in BM25/QPS index.

2. **QPS > BM25 for short queries** — Quantum Proximity Scoring tokenises into "quantums" and scores adjacency. For the 2-5 word queries typical in vault search, this dramatically improves relevance over BM25's bag-of-words approach.

3. **No headless search API in Obsidian** — `prepareFuzzySearch()` and `prepareSimpleSearch()` are the only official search functions. They're low-level string matchers, not the full search panel experience. The internal `global-search` plugin requires the UI panel to be open and `setTimeout` guessing for results.

4. **MCP tool descriptions are critical** — they're the only thing AI clients see when deciding how to use a tool. Must be comprehensive: list all parameters, example values, explain what each mode does, mention what requires Ollama vs what works without it.

5. **Orama `enum[]` with `containsAny`/`containsAll`** — the right primitive for tag filtering. Combined with `where` clause logical operators, this gives us the filtering power we need without building a custom query parser.

### Statistics

- 7 files modified, 5 new files created
- `src/chunker.ts`: 213 lines (new)
- `test/unit/chunker.test.ts`: 251 lines (new)
- `docs/features/unified-search.md`: 265 lines (new)
- 67/68 tests passing (1 pre-existing integration test failure)
- 19 new chunker tests + updated vector store tests

### What's Next

- **Unified search implementation** — Step 1: Engine abstraction, Step 2: Schema v5 + QPS, Step 3: Unified `search` tool, Step 4: `find` tool, Step 5: Update search panel, Step 6: Remove old tools
- Background indexing with status bar indicator
- Chaos triage MCP tools

### Reflections

This session was as much about architecture as implementation. The chunker was the coding deliverable, but the real value came from the deep exploration of search options — systematically comparing brute-force grep, Obsidian's API, and Orama's capabilities to arrive at a clear, well-reasoned architecture. The user's discovery of QPS was the final piece that made the all-in-Orama approach compelling. Sometimes the most productive sessions are the ones where you spend more time thinking than typing.

---

## 2026-02-08 - Unified Search: From Spec to Implementation

**Objective**: Implement the unified search feature spec from the previous session — consolidate three MCP tools into two, switch from BM25 to QPS, add tag/path filtering, and make all files searchable regardless of embedding success.

### What We Built

The full unified search architecture, from spec to working code:

1. **`SearchEngine` interface** (`src/search-engine.ts`) — clean abstraction over search implementations, decoupling consumers from Orama
2. **`OramaSearchEngine`** — renamed from `VectorStore`, implements the interface, schema v5 with QPS, tags, folders, two-phase indexing
3. **Unified `search` MCP tool** — replaces the old brute-force `search` and `semantic_search` tools. Hybrid/vector/fulltext modes, tag and path filtering, quoted phrase support, structured JSON responses
4. **`find` MCP tool** — replaces `find_files` with richer metadata. Pattern, path, tag, and frontmatter property filtering
5. **Two-phase indexing** — Phase 1 indexes content + metadata (always succeeds), Phase 2 generates embeddings (may fail, file stays fulltext-searchable)
6. **23 unit tests** covering QPS scoring, optional embeddings, tag/folder metadata, schema versioning
7. **Two feature specs** for next-up enhancements: phrase search boosting and find tool sorting

### The Journey

The session started with a clear mandate: "go ahead, start implementing unified search." The previous session had produced a detailed spec in `docs/features/unified-search.md`, so the path was well-mapped.

**Systematic execution.** Set up a 7-step todo list mirroring the spec's implementation plan: install QPS, create interface, refactor VectorStore, replace MCP tools, update search panel, update tests, verify. Each step built on the last with no backtracking.

**The VectorStore refactor** was the meatiest change. Renamed to `OramaSearchEngine`, bumped schema to v5, added `tags: 'enum[]'` and `folder: 'enum'` fields, wired in `pluginQPS()`, and rewrote `indexFiles()` to support two-phase indexing with an options object. The key insight from the spec — that Orama schema fields are optional at insert time — made the two-phase approach clean: just omit the `embedding` field and the document stays in the QPS/fulltext index.

**Three tools became two.** The old brute-force `search` (77KB of unranked grep output), `semantic_search` (Orama-backed but separate), and `find_files` (filename-only) were replaced by a unified `search` (all three modes + filtering + structured JSON) and `find` (vault API + metadataCache for rich metadata). The new `search` tool auto-initialises the engine and auto-indexes stale files on first use.

**Testing was smooth.** Updated all 23 unit tests for the new schema, added suites for documents without embeddings, tag/folder metadata, and QPS proximity scoring. All passed on first run. The integration tests were updated for new tool names but expectedly fail against the old deployed plugin — they'll pass once the new build is deployed.

**Deployed to main vault** for manual testing. The schema version bump (v3 → v5) triggers automatic re-indexing.

**Feature planning.** After testing, the user identified two enhancement areas: phrase search quality (exact phrases should rank higher) and find tool sorting (sort by frontmatter dates). Rather than scope-creep the working implementation, we captured both as feature specs with detailed implementation plans for future sessions.

### Technical Achievements

1. **Clean engine abstraction** — `SearchEngine` interface means the Orama implementation can be swapped without touching consumers
2. **No blind spots** — every markdown file in the vault is now fulltext-searchable, even if Ollama is down or embedding fails
3. **14 MCP tools** — consolidated from 15, but more capable. The `search` tool alone replaces two previous tools with better filtering and structured output
4. **Schema v5** — tags and folder metadata stored in Orama for efficient `where` clause filtering
5. **Backwards compatibility** — `export const VectorStore = OramaSearchEngine` alias ensures no breakage during migration

### Key Learnings

1. **QPS is a drop-in BM25 replacement** — `pluginQPS()` slots into `create()` with zero API changes. Orama's search API is the same; only the scoring algorithm changes. This made the switch trivial.

2. **Two-phase indexing is the right pattern** — separating content indexing from embedding generation means failures in one phase don't block the other. The `indexFiles()` options object (`generateEmbeddings`, `minContentLength`, `getFileTags`) keeps the API clean.

3. **Feature specs pay dividends** — having a detailed spec from the previous session meant this implementation session was pure execution. No architecture debates, no design dead-ends. The spec's step-by-step plan mapped directly to the todo list.

4. **Scope discipline matters** — the user correctly identified that phrase boosting and find sorting were separate features, not part of the current task. Creating feature specs instead of implementing them kept the PR focused and the working code stable.

5. **Orama has no native phrase search** — investigated thoroughly. No phrase parameter, no exact phrase mode, no proximity search beyond what QPS provides. The current `filterByPhrases` post-filter approach is the only option; the phrase-search-boosting spec improves it from binary filter to boost.

### Statistics

- 12 files changed, 818 insertions, 396 deletions (unified search commit)
- 1 new file created: `src/search-engine.ts` (66 lines)
- 2 feature specs created: `phrase-search-boosting.md`, `find-sorting.md`
- 37 unit tests passing (14 chunker + 23 vector-store)
- 3 MCP tools removed, 2 added (net -1: 15 → 14 tools)
- Schema version: v3 → v5

### What's Next

- **Phrase search boosting** — replace binary `filterByPhrases` with `boostByPhrases` that partitions results (phrase matches first, rest after) using full chunk content
- **Find tool sorting** — add `sortBy` parameter with auto-detected date/datetime properties
- **Manual testing** — verify unified search on the 4,097-document main vault
- **Background indexing** — automatic incremental indexing with status bar indicator
- **Chaos triage** — MCP tools for processing chaos items one at a time

### Reflections

This was a satisfying "spec → implementation" session. The previous session's exploration and spec work meant today was about methodical execution rather than exploration. Each of the 7 implementation steps completed cleanly, the build passed on first attempt, and all unit tests passed. The feature branch approach (`feature/unified-search`) keeps main clean while we verify the changes in the real vault.

The user's instinct to stop and capture the phrase boosting and find sorting as separate feature specs rather than tacking them onto this PR was good engineering discipline. The unified search is a solid foundation; those enhancements can land independently without risking what's already working.

---

## 2026-02-08 - Search Refinements & Panel Filters

**Objective**: Polish the search experience — enrich the `find` tool response, implement phrase search boosting and find tool sorting from existing specs, and add path/tag filtering to the search panel sidebar.

### What We Built

This was a multi-feature session spanning four distinct improvements across two extended conversations:

1. **Phrase search boosting** — replaced the binary `filterByPhrases` with `boostByPhrases` that partitions results into phrase-match and rest groups, keeping all results instead of discarding non-matches
2. **Find tool sorting** — added `sortBy` parameter with auto-detected ISO date properties, configurable direction, and nulls-to-end
3. **Find tool response enrichment** — added title, full frontmatter, and lowered default limit from 50 to 20
4. **Search panel filters** — added path and tag filtering to the sidebar search panel using Obsidian's `AbstractInputSuggest` with fuzzy autocomplete

### The Journey

The session began by implementing the two feature specs from the previous session — phrase boosting and find sorting. Both had detailed step-by-step plans which made implementation straightforward.

**Phrase boosting** was a clean partition-and-concatenate approach. After Orama returns results, split into two groups: results where the full chunk content contains ALL quoted phrases, and results where it doesn't. Phrase matches appear first, preserving score order within each group. The key insight was over-fetching (3x the limit) when phrases are present, since QPS might rank a phrase match low even though it's the most relevant result.

**Find tool sorting** required auto-detecting ISO dates from the actual data. Rather than asking users to declare types, we sample the first 20 files and check if values match `/^\d{4}-\d{2}-\d{2}/`. Dates default to `desc`, strings to `asc`. Nulls always sort to end regardless of direction. ISO date strings sort correctly with `localeCompare`, so no Date parsing is needed.

After deploying to the main vault and testing via MCP, the user noticed the `find` tool response was too sparse — just paths. We enriched it with title (`file.basename`), full frontmatter (excluding Obsidian's internal `position` key), tags, size, and mtime. The default limit dropped from 50 to 20 since this is primarily an AI-consumed tool.

The final feature — **search panel filters** — was the most interesting from a UI perspective. Research into Obsidian's API revealed `AbstractInputSuggest`, a built-in class for inline autocomplete popovers that handles keyboard navigation, scrolling, and positioning automatically. Combined with `prepareFuzzySearch()` and `renderResults()` for highlighted fuzzy matching, it provided a polished autocomplete experience with minimal code.

Two suggest classes were created: `FolderSuggest` (using `vault.getAllFolders()`) and `TagSuggest` (using the undocumented but efficient `metadataCache.getTags()` with a fallback to `getAllTags()`). Selected filters appear as removable chips below the inputs. Adding or removing a filter re-triggers the current search.

### Technical Achievements

- **`AbstractInputSuggest` integration** — first use of Obsidian's built-in autocomplete in the plugin, providing native-feeling folder and tag suggestions
- **Fuzzy-highlighted suggestions** — `prepareFuzzySearch()` + `renderResults()` gives users the same highlighting they see in Obsidian's core search
- **Undocumented API discovery** — `(app.metadataCache as any).getTags()` returns all vault tags with counts in a single call, much more efficient than iterating every file
- **Clean partition boosting** — phrase matches always appear first without tuning magic score bonuses
- **ISO date auto-detection** — sort direction inferred from data, no schema declarations needed

### Key Learnings

1. **`AbstractInputSuggest` is the right tool for sidebar autocomplete** — not a modal, positions near the input, handles keyboard nav and scrolling automatically. Works perfectly in sidebar panels.

2. **Obsidian's `SearchResult` type collides with ours** — needed to alias the import as `ObsidianSearchResult` to avoid conflicts with our own `SearchResult` from `search-engine.ts`.

3. **`metadataCache.getTags()` is undocumented but stable** — returns `Record<string, number>` with tag names as keys and counts as values. Much faster than iterating all files.

4. **Over-fetching is essential for phrase boosting** — without 3x the limit, phrase matches that QPS scored low get cut off before the boost can promote them.

5. **Frontmatter's `position` key is internal** — Obsidian adds this to `cache.frontmatter` for internal tracking. Must be excluded when sending frontmatter to MCP clients.

### Statistics

- 12 commits across two conversation sessions
- 21 files changed, 2,664 insertions, 442 deletions (from v0.7.0 baseline)
- 4 feature specs created/completed
- 3 feature branches merged to main
- 37 unit tests passing (14 chunker + 23 vector-store)
- 2 new `AbstractInputSuggest` subclasses
- Search panel: 3 UI elements added (2 filter inputs + chip container)

### What's Next

- **Background indexing** — automatic incremental indexing with status bar indicator
- **Chaos triage** — MCP tools for processing chaos items one at a time
- **Re-ranking** — optional Ollama-based re-ranking for higher precision results

### Reflections

This session demonstrated the value of the spec-first approach. Four features landed cleanly because each had a clear plan before any code was written. The phrase boosting and find sorting specs from the previous session were pure execution — no design debates, no dead ends. The search panel filters feature went from research (discovering `AbstractInputSuggest`) to spec to implementation to deployment in a single flow.

The user's instinct to test features against the real 4,097-document vault via MCP before considering them done has been invaluable. It's caught issues like sparse response payloads and sorting edge cases that unit tests wouldn't surface.

---

*End of log entry*

## 2026-02-10 - File Counts, Light Moves, and Periodic Reconciliation

**Objective**: Improve background indexing accuracy — show correct file counts, handle renames efficiently, and add a safety net for missed events.

### What We Built

- **File count tracking**: Status bar and settings now show the number of unique files indexed (e.g. "1,874 files indexed") instead of the Orama chunk count, which was a misleading internal metric
- **Light file moves**: File renames/moves update index metadata (path, title, folder, chunk IDs) without re-generating embeddings via Ollama — O(n chunks) remove+insert vs a full embedding round-trip
- **Periodic reconciliation**: A bidirectional scan every 60 seconds that catches stale files (forward: vault → index) and orphaned entries (reverse: index → vault)
- **Removed sync-settling**: The complex `waitForSync()`/`syncSettling`/`pendingSyncActions` mechanism was replaced entirely by reconciliation, which naturally handles Obsidian Sync drift

### The Journey

The session started with a seemingly simple request: show file count instead of chunk count in the status bar. This led to adding an `indexedFiles` Set to the vector store, persisted in the save envelope. Straightforward.

Then came the question about moves — "do moves matter?" Yes, but more importantly, a rename shouldn't trigger a full re-index with embedding generation. So we added `moveFile()` to the vector store, which does O(1) lookups per chunk via `getByID()`, removes the old entry, and re-inserts with updated metadata. The embedding vector is preserved as-is. The `IndexQueue` gained a 'rename' action type with `oldPath` tracking, and `processQueue()` handles renames with a fallback to full indexing if the old path isn't found.

After deploying to the main vault, problems emerged. The status bar was stuck on "Witness loading" for ages. Investigation of the indexing logs revealed Readwise was the culprit — re-indexing ~60 files every 30 minutes, 3,015 files total across 67 batches in a single day. The modify events were legitimate (Readwise touches file mtimes), but the re-embedding was wasteful.

More critically, disabling the plugin, deleting files, and re-enabling left orphaned entries in the index. The event-driven system had no way to detect what happened while it was off.

This led to the key architectural decision: add periodic reconciliation as a safety net. The debate was whether to replace event listeners entirely with polling. The answer: keep both. Event listeners provide instant responsiveness (3-second debounce), while reconciliation catches everything that slips through — sync drift, offline mutations, plugin restarts, Readwise storms.

The implementation was clean:
- `getOrphanedPaths()` on the vector store compares `indexedFiles` against vault paths
- `reconcile()` does both directions in one pass, then saves
- `startBackgroundIndexing()` replaces the complex `startupIndexing()` — init, clear queue, reconcile once, start 60-second timer
- All sync-settling code removed (6 fields/methods eliminated)

One subtle bug emerged after deployment: the status bar showed "516 files indexed" after reconciling 516 stale files, when the vault has 1,874 files. The issue: `getStaleFiles()` iterated all files but only tracked stale ones — files already up-to-date in the index were never added to `indexedFiles`. A two-line fix (adding `this.indexedFiles.add(file.path)` in the "not stale" branches) resolved it.

### Technical Achievements

- **`indexedFiles` Set**: Tracks unique file paths across all operations (index, remove, move, clear), persisted in the save envelope, and self-healing via `getStaleFiles()`
- **Light moveFile**: Preserves embeddings during renames — zero Ollama calls for file moves
- **Bidirectional reconciliation**: Forward scan (stale detection) + reverse scan (orphan detection) in a single pass
- **Sync-settling elimination**: Removed ~80 lines of complex async sync-waiting logic, replaced by a simple 60-second timer

### Key Learnings

- **Tracking state across saves**: When adding a new tracking mechanism (like `indexedFiles`), the first deploy against an existing index won't have the new data in the envelope. The system must self-heal — `getStaleFiles()` now populates the Set as a side effect, so it converges after one reconciliation pass.
- **Event-driven + polling is better than either alone**: Events for immediacy, polling for correctness. Neither is sufficient on its own when the system can be paused, restarted, or bypassed.
- **Readwise storm diagnosis**: Log files were essential. Without the `indexing-YYYY-MM-DD.log`, the Readwise re-indexing pattern (every 30 minutes, ~60 files) would have been invisible. The user fixed it on the Readwise side, but the reconciliation architecture makes Witness resilient to similar future issues.
- **Status bar as contract**: Users read the status bar as the source of truth. If it says "516 files indexed" when 1,874 exist, trust is broken. The fix was trivial but the principle matters — every public-facing number must reflect the full state, not just the last operation.

### Statistics

- **Files changed**: 5
- **Lines**: +291 / -107
- **Tests**: 37/37 passing (14 chunker + 23 vector-store)
- **Deployed**: Both demo vault and main vault (1,874 files)
- **Readwise storm eliminated**: From 3,015 redundant re-indexes/day to 0

### Next Steps

- Handle the context-length embedding failures (several files consistently fail — investigate chunker behaviour for long documents)
- Consider adding a `semantic_search` toggle to settings UI (the `enableSemanticSearch` field exists but has no UI control yet)
- Log viewer in settings tab (from the background-indexing spec — not yet implemented)
- Monitor reconciliation performance on the main vault over the next few days

### Reflections

This session demonstrated the value of deploying to a real vault early. The Readwise storm, orphaned entries, and status bar count bug were all invisible in the demo vault and unit tests. The main vault — with 1,874 files, Obsidian Sync, Readwise integration, and actual daily use — is where the real edge cases live.

The reconciliation pattern is satisfying in its simplicity. Instead of trying to handle every possible source of drift (sync, plugin restart, external tools), we just check periodically. The event listeners handle the 99% case instantly; reconciliation handles the 1% eventually. Both are necessary; neither is sufficient alone.

---

*End of log entry*

## 2026-02-08 - Chaos Triage: Teaching the AI to Process Unread Items

**Objective**: Implement MCP tools for triaging chaos items — the first step toward Witness actively helping manage the chaos→order pipeline — along with safety improvements to prevent AI tools from accidentally destroying files.

### What We Built

1. **`get_next_chaos` MCP tool** — Scans the chaos folder for untriaged items, filtering by triage status and deferred dates. Two modes: single (full content of next item) and list (compact path/title/date for up to 10 items). Includes queue counts showing total pending and in-path counts.

2. **`mark_triage` MCP tool** — Records triage decisions using Obsidian's `processFrontMatter` API for safe YAML updates. Three actions: `processed` (stamps today's date), `deferred` (boomerang with target date), `acknowledged` (reviewed, no action needed).

3. **`write_file` made create-only** — Previously silently overwrote existing files. Now errors with guidance toward `edit_file`. This prevents the AI's worst instinct: delete and recreate when confused.

4. **`edit_file` error messages improved** — When text isn't found, the error now truncates the search string (max 80 chars), suggests re-reading the file, and warns against delete+recreate.

5. **Link discovery feature spec** — Wrote up the full spec for future two-stage pipeline (embeddings for cheap candidate detection, local LLM for validation/anchor text, Grammarly-style review panel).

6. **Chaos triage feature spec updated** — Renamed `triage_chaos` → `mark_triage`, added list mode, queue counts, and the edit_file guidance section.

### The Journey

The session started with writing up the link discovery feature spec, but quickly pivoted to the more pressing need: chaos triage. The user had been using Witness daily and noticed two friction points that needed addressing before triage tools would work reliably.

**The Delete+Recreate Problem**

The most interesting finding was around AI behaviour. When the AI encounters a frontmatter editing challenge — YAML is fiddly, indentation matters, and metadata cache can lag — it often falls back to deleting the file and recreating it from scratch. This "works" in the sense that the content is correct, but destroys file metadata (creation date, mtime) and any Obsidian-internal state (backlinks cache, etc.). The fix was twofold:

1. Make `write_file` refuse to overwrite existing files, with an error message that guides toward `edit_file`
2. Make `edit_file` errors explain what probably went wrong and warn against the delete+recreate pattern

Both changes are essentially "AI guardrails" — the tools now teach the AI how to recover from mistakes instead of letting it fall into destructive patterns.

**The `processFrontMatter` Discovery**

For `mark_triage`, the key insight was using Obsidian's `processFrontMatter` API instead of having the AI manually edit YAML. This API accepts a callback that receives the parsed frontmatter object — set a property, done. No YAML parsing, no indentation issues, no race conditions with the metadata cache. This is exactly the kind of specialised tool that prevents the fumbling.

**List Mode Context Budget**

The first implementation of list mode returned full frontmatter for all pending items. The user immediately reported it killed their context window — a 4,000-file vault produces a lot of metadata. Slimmed it to just `{path, title, date}` with a max of 10 items. The lesson: always think about what the AI actually needs to make a decision, not what might be useful.

### Technical Achievements

- **16 MCP tools** now registered (up from 14)
- **Triage frontmatter convention**: `triage: YYYY-MM-DD` (processed), `triage: deferred YYYY-MM-DD` (boomerang), `triage: acknowledged` (reviewed)
- **Deferred date comparison**: String comparison `deferDate <= today` works for YYYY-MM-DD format — no Date parsing needed
- **Queue counts**: Every response includes `{total, in_path}` so the AI knows how much work remains
- **47 integration tests** passing (up from 37), including 11 new triage tests
- **37 unit tests** passing

### Key Learnings

- **AI guardrails in error messages work**: The `write_file` and `edit_file` error messages now teach the AI the correct workflow. This is more effective than documentation — the AI reads errors in real-time.
- **`processFrontMatter` is the right abstraction**: Obsidian's API for safe frontmatter updates eliminates an entire class of YAML-fumbling bugs. Always look for platform APIs before rolling your own.
- **Context window budget matters for list endpoints**: Returning "everything" from a list endpoint can blow the AI's context window. Design list responses with the minimum fields needed for the next decision.
- **Deferred dates as string comparison**: YYYY-MM-DD format is lexicographically ordered, so `"2026-02-01" <= "2026-02-08"` just works. No need for Date objects.

### Statistics

- **Files changed**: 6 (+ 7 new test fixtures, + 1 new feature spec)
- **Lines**: +445 / -54
- **Integration tests**: 47 passing (was 37)
- **Unit tests**: 37 passing
- **MCP tools**: 16 registered (was 14)
- **Deployed**: Test vault, demo vault, and main vault

### Next Steps

- Fix background indexing stutter (typing lag every 30 seconds when reconciliation runs)
- Re-ranking feature (Ollama-based re-ranking for higher precision results)
- Link discovery implementation

### Reflections

This session was about making the AI a better collaborator. The triage tools themselves are straightforward — scan, filter, sort, update frontmatter. What's more interesting is the defensive design: error messages that teach, tools that refuse destructive shortcuts, and response formats tuned to context window budgets. Every tool is a conversation with the AI, and the error paths are where the real teaching happens.

---

*End of log entry*

## 2026-02-08 - Idle-Gated Indexing & Status Bar Countdown

**Objective**: Eliminate UI stutter during background indexing by replacing the web worker approach with app-wide idle detection, and add a visible countdown timer so the user can see when indexing will run.

### What We Built

- App-wide idle detection: indexing only runs after 2 minutes of complete inactivity (no mouse, keyboard, clicks, or scrolling)
- Debounced index saves: instead of serialising 279 MB after every change, saves are batched on a 30-second timer
- Active file deferral: the file currently being edited is skipped and re-queued for later
- Status bar countdown timer: shows "3 pending, indexing in 1:45" when work is waiting
- Removed the web worker (orama-worker.ts, orama-worker-client.ts) — simpler architecture, same result
- Removed setTimeout(0) yields from vector-store.ts — no longer needed with idle gating

### The Journey

This session started as a web worker implementation session. The previous conversation had built the full worker infrastructure: orama-worker.ts (IIFE worker), orama-worker-client.ts (Promise-based postMessage wrapper), esbuild worker build, and a rewritten vector-store.ts that proxied all Orama operations through the worker.

But testing revealed a fundamental problem: even with Orama running off-thread, the main thread still did significant work — reading files from the vault, chunking markdown, calling Ollama for embeddings, and crucially, transferring the 279 MB serialised index back via postMessage's structured clone algorithm. The structured clone of a 279 MB object is itself a blocking operation.

The user asked the right question: "why isn't all of this in the worker?" The answer was that Obsidian's vault APIs (cachedRead, metadataCache, workspace) are main-thread-only — they can't be called from a worker. So the architecture was fundamentally split: worker owns the DB, main thread owns the I/O. And the bridge between them (postMessage) was the bottleneck.

Several alternatives were explored:
- **IndexedDB in the worker**: Would avoid the main-thread save entirely, but the user rejected this because it's hidden and hard to debug. A JSON file you can inspect, delete, and share is more practical.
- **Embeddings-only persistence**: Only save the expensive-to-regenerate embeddings, rebuild everything else from vault files on startup. Interesting idea but adds startup latency.
- **Idle detection**: The user's insight — "the stutter is only a problem if I'm working on Obsidian" — cut through all the complexity. If you simply don't do any indexing work while the user is active, the stutter disappears regardless of how much main-thread work is involved.

This led to the architectural pivot: remove the web worker entirely and replace it with app-wide idle detection. The implementation is dramatically simpler:

1. **DOM event listeners** on `document` (mousemove, keydown, click, scroll) track `lastAppActivity`
2. **`waitForIdle()`** polls every 10 seconds, resolves when 2 minutes of silence have passed
3. Both `processQueue()` and `reconcile()` call `waitForIdle()` before doing any work
4. Initial startup skips the idle wait (`reconcile(true)`) so search is immediately available

On top of that, three optimisations were added back:
- **Debounced save**: A dirty flag + 30-second timer replaces the per-operation save. Flushed on plugin unload.
- **Active file deferral**: The file the user is editing is filtered out and re-queued, avoiding re-indexing a file that's about to change again.
- **`isUserActive` safety net**: The existing 500ms pause loop inside `indexFiles()` was kept as a belt-and-suspenders check, now using `!isAppIdle()` instead of the old editor-only activity check.

A bug emerged immediately: the initial reconciliation hung forever in `waitForIdle()` because the user was actively working in Obsidian. Fixed by adding a `skipIdleWait` parameter that's `true` only for the startup call.

The final feature — the status bar countdown — gives the user visibility into the idle gating behaviour. When work is waiting and the user is active, the status bar shows something like "Witness: 3 pending, indexing in 1:45". The countdown resets on every interaction and ticks every 5 seconds. A `waitingForIdle` flag tracks whether any code path is currently blocked in `waitForIdle()`, which covers both queue-triggered and reconcile-triggered waits.

### Technical Achievements

- **Zero-stutter indexing** — main thread is never blocked by indexing because indexing only runs when the user isn't there
- **279 MB save debouncing** — instead of serialising the full index after every file change, saves are batched to at most once per 30 seconds
- **Visible idle countdown** — users can see exactly when indexing will run and adjust the threshold if needed
- **Removed ~400 lines of worker code** — simpler architecture with the same user experience
- **37 unit tests passing** — all existing tests continue to pass

### Key Learnings

1. **UX solutions can beat technical solutions**: Moving Orama to a worker was technically correct but didn't solve the root problem (structured clone of 279 MB). Idle detection is simpler, more robust, and actually eliminates the perceived issue.
2. **The user doesn't care about thread scheduling**: They care about stutter. If work only happens when they're not looking, the stutter doesn't exist. This is a profoundly different framing than "make the work faster".
3. **Visibility builds trust**: The countdown timer transforms the idle gate from an invisible heuristic into a visible feature. Users can see exactly what's happening and why indexing hasn't run yet.
4. **postMessage is not free**: Structured cloning large objects across worker boundaries is itself a blocking main-thread operation. Workers help with CPU-bound work, not data-transfer-bound work.
5. **DOM events are better than editor events**: The old `editor-change` listener only tracked typing. App-wide DOM events (mousemove, keydown, click, scroll) catch all forms of interaction — scrolling through notes, clicking links, navigating the file tree.

### Statistics

- 2 files changed: src/main.ts (+199, -26), src/vector-store.ts (-8)
- 2 files deleted: src/orama-worker.ts, src/orama-worker-client.ts
- 37 unit tests passing
- Net complexity reduction: ~400 lines of worker code removed, ~175 lines of idle detection added

### Next Steps

- Re-ranking feature (Ollama-based re-ranking for higher precision results)
- Link discovery implementation
- Consider making the idle threshold configurable in settings

### Reflections

This session is a case study in the value of stepping back from a technical approach. The web worker was the "obvious" engineering solution — move heavy work off the main thread. But the real insight was that indexing doesn't need to be fast, it just needs to be invisible. A 2-minute idle threshold is practically free in terms of user experience, and it eliminates an entire class of complexity (worker lifecycle, message passing, structured cloning, split architecture). Sometimes the best optimisation is not doing the work at all.

---

*End of log entry*

## 2026-02-08 — LLM Re-ranking & UX Polish

**Objective**: Implement Ollama-based LLM re-ranking for search results, making search precision optionally higher for complex queries.

### What We Built

- **LLM Re-ranking Pipeline**: Two-stage search — fast hybrid search (stage 1, top-30) followed by LLM relevance scoring (stage 2, top-K)
- **OllamaProvider Extensions**: `generate()`, `listChatModels()`, `rerank()`, and `parseRerankResponse()` methods
- **MCP Integration**: `search` tool gains `rerank: true` parameter for AI clients
- **Settings UI**: Re-ranking section with enable toggle, model dropdown, and suggested model pull buttons (llama3.2:1b, qwen2.5:1.5b, phi3:mini, gemma2:2b)
- **Search Panel Toggle**: Checkbox in sidebar, hidden when re-ranking isn't configured
- **Two-Phase Search UX**: When re-ranking is enabled, stage 1 results appear instantly with an animated "Re-ranking with LLM..." banner, then re-ranked results replace them
- **Animated Status Banner**: Accent-coloured banner with cycling dots animation during re-ranking, transitions to muted "Re-ranked in Xms" when done
- **Enter-Only Mode**: Debounce disabled when re-rank is on (too expensive per keystroke), placeholder updates to hint "press Enter"
- **Race Condition Guard**: Sequence counter prevents stale search results from overwriting newer ones
- **Configurable Idle Threshold**: Moved hardcoded 2-minute idle detection threshold to settings (Indexing Filters section)

### The Journey

The session started with a clear plan from a previous planning phase — the architecture was already designed with the two-stage pipeline, batch prompt approach, and graceful degradation strategy. Implementation went smoothly through the seven planned steps: OllamaProvider methods, settings, search pipeline, MCP tool, settings UI, sidebar toggle, and build/test.

The interesting part was the UX iteration. After the initial deployment, the user noticed several issues:

1. **Settings grouping**: The "Enable re-ranking" and "Re-ranking model" settings were rendered as standalone `new Setting()` calls instead of using `SettingGroup.addSetting()`. Same issue existed for the Ollama Connection section (Base URL + Embedding Model). Fixed both to use consistent grouping.

2. **Debounce + re-ranking clash**: The 300ms debounce was firing LLM calls on every typing pause — wasteful and jarring. Switched to Enter-only when re-rank is enabled, with the placeholder text updating as a hint.

3. **No feedback during re-ranking**: The LLM call takes 1-5+ seconds with no indication of progress. The first attempt was a simple "Searching + re-ranking..." loading state, but the user wanted more — they wanted to know what was happening, not just that something was happening.

The breakthrough was the two-phase UX: show stage 1 results immediately (the user can start reading), then update when re-ranking completes. The animated dots banner ("Re-ranking with LLM.") cycling through one, two, three dots gives a clear sense of activity, and the accent colour makes it unmissable. When done, it transitions to a muted "Re-ranked in 2341ms" showing actual timing.

1. **Quality concerns**: The user tested with multiple models and found re-ranking results "not great." Small chat models (1-4B) doing batch relevance scoring is the weakest form of re-ranking — they tend to give everything similar scores. Dedicated cross-encoder models (bge-reranker, jina-reranker) are purpose-built for this but Ollama doesn't expose a native rerank endpoint yet. Documented this honestly rather than over-promising fixes.

2. **Minor polish**: Added padding between the "Available Re-ranking Models" heading and the settings above it (12px margin-top on rerankContent div).

The idle threshold was a quick bonus — replaced the hardcoded `static readonly IDLE_THRESHOLD_MS = 120_000` with a settings-backed getter, using `parseFloat` to support fractional minutes.

### Technical Achievements

- **535 lines added** across 5 files with clean build and all 37 unit tests passing
- Graceful degradation at every level: JSON parse → regex fallback → original order
- Two-phase search UX with no architectural changes to the search pipeline (just calls `search()` twice in the view layer)
- CSS-only styling with Obsidian theme variables (works in light and dark mode)
- JS-based dots animation (CSS `content` in `@keyframes` doesn't work in most browsers — caught this before deploying)

### Key Learnings

- **Small LLMs as relevance judges have real limitations**: The LLM-as-judge pattern works well with large models (GPT-4, Claude) but 1-4B local models lack the nuance for fine-grained relevance scoring. Cross-encoder reranking models are purpose-built for this.
- **Two-phase UX > loading spinners**: Showing real results immediately while the slow operation runs in the background is strictly better than a spinner. Users can start working while waiting.
- **SettingGroup consistency matters**: Standalone `new Setting()` calls look visually disconnected from their group heading. Always use `group.addSetting()` for settings that belong together.
- **Debounce assumptions break with expensive operations**: A 300ms debounce is fine for <100ms searches but terrible for 3+ second LLM calls. The UX should adapt to the expected latency of the operation.
- **CSS `content` animation is unreliable**: The `@keyframes` approach for animating `content` (cycling dots) doesn't work in most browsers. Use JS `setInterval` instead.

### Statistics

- 3 commits on `feature/reranking` branch
- 5 files changed, 535 insertions, 78 deletions
- 4 source files modified: main.ts (+311/-68), ollama-provider.ts (+154), search-view.ts (+107/-16), styles.css (+37)
- All 37 unit tests passing

### Next Steps

- Link discovery — the next major feature on the TODO
- Monitor Ollama for native rerank endpoint support (would dramatically improve re-ranking quality)
- Consider whether re-ranking should be kept as-is or removed given quality limitations

### Reflections

This session was a masterclass in UX-driven iteration. The initial implementation was technically complete but the user experience wasn't right — the debounce was wrong, the feedback was missing, the settings looked inconsistent. Each piece of feedback led to a targeted improvement. The two-phase search UX was the biggest win: it's such a natural pattern (show what you have, refine in the background) but it wouldn't have emerged without the user flagging the lack of feedback. Sometimes the feature isn't done when the code works — it's done when it feels right.

The quality discussion was also important. Being honest about the limitations of small-model re-ranking (rather than endlessly tweaking prompts) is the right call. The infrastructure is there for when better reranking options emerge.

---

*End of log entry*
